# biasbenchmark
In this study, we set out to measure race and gender bias prevalent in text-to-image (TTI) AI image generation, focusing on the popular model Stable Diffusion from Stability AI. Previous investigations into the biases of word embedding models—which serve as the basis for image generation models—have demonstrated that models tend to overstate the relationship between semantic values and gender, ethnicity, or race. These biases are not limited to straightforward stereotypes; more deeply rooted biases may manifest as microaggressions or imposed opinions on policies, such as paid paternity leave decisions. In this analysis, we use image captioning software OpenFlamingo and Stable Diffusion to identify and classify bias within text-to-image models. Utilizing data from the Bureau of Labor Statistics, we engineered 50 prompts for profession and 50 prompts for actions in the interest of coaxing out shallow to systemic biases in the model. Prompts included generating images for "CEO", "nurse", "secretary", "playing basketball", and "doing homework". After generating 20 images for each prompt, we document the model’s results. We find that biases do exist within the model across a variety of prompts. For example, 95% of the images generated for "playing basketball" were African American men. We then analyze our results through categorizing our prompts into a series of income and education levels corresponding to data from the Bureau of Labor Statistics. Ultimately, we find that racial and gender biases are present yet not drastic.
